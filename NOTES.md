# ğŸ§  The AI Engineering Challenge: Technical Notes

This document provides a deep dive into the theoretical concepts, architecture, and cost considerations for building the **ContentNavigatorAI** RAG system.

## ğŸ› ï¸ Theoretical Considerations

### 1. The RAG Acronym Paradox

**Runtime Focus: The R-A-G Flow**
**RAG** (**Retrieval-Augmented Generation)** is a **_method_**, and **LangChain** is a **framework**, which can implement it.

The name "Retrieval-Augmented Generation" (RAG) is pretty much based on the sequence of operations that happen at runtime after the initial knowledge base is built:

**System Build Focus: The A-R-G Flow**

1. **Retrieval** (R) must happen first.
2. The retrieved documents **Augment** (A, or provide context for) the prompt.
3. The LLM then performs the **Generation** (G).

From the system build focus, which follows the execution order, is much like A-R-G.

- **A - Augmentation/Indexing (The Build Phase)**: This is the mandatory preparation step before any user query is processed. Your code correctly labeled this as the Indexing Phase.
- **R - Retrieval (The Search Phase)**: This is the first step that runs during a user query, searching the index created in A.
- **G - Generation (The Answer Phase)**: This is the final step that runs during a user query, using the context retrieved in R to generate the answer.

Taken together, the acronym order (RAG) is runtime-focused, while the system build focus, which follows the execution order, is much like A-R-G.

<br></br>

### 2. Core Components of the RAG System

The listed order of components also follows the steps in the Augmentation (A) / Initialization phase of a RAG system.

![The RAG Build Phase: From Raw Text to Vector Index](/assets/ai_engineering_rag-build-phase.drawio.png)  
ğŸ–¼ï¸ open the image in a separate window:
[The RAG Build Phase: From Raw Text to Vector Index](/assets/ai_engineering_rag-build-phase.drawio.png)

### A. CUSTOM DATA ğŸ“

#### ğŸ”§ **Example Source**: Uploaded PDF / JSON / etc. files; or other data sources.

#### ğŸ’¡ **Example Custom Data**:

```txt
âœ… 1. PROVIDED CUSTOM TEXT

The hippocampus (pl.: hippocampi; via Latin from Greek á¼±Ï€Ï€ÏŒÎºÎ±Î¼Ï€Î¿Ï‚, 'seahorse'), also hippocampus proper, is a major component of the brain of humans and many other vertebrates. In the human brain the hippocampus, the dentate gyrus, and the subiculum are components of the hippocampal formation located in the limbic system. The hippocampus plays important roles in the consolidation of information from short-term memory to long-term memory, and in spatial memory that enables navigation. In humans and other primates the hippocampus is located in the archicortex, one of the three regions of allocortex, in each hemisphere with direct neural projections to, and reciprocal indirect projections from the neocortex. The hippocampus, as the medial pallium, is a structure found in all vertebrates.

....
```

[example text source: Wikipedia](https://en.wikipedia.org/wiki/Hippocampus)

### B. TEXT SPLITTER âœ‚ï¸

#### ğŸ”§ **Example Tool**:

`RecursiveCharacterTextSplitter`

#### âš™ï¸ **How it Works**:

It creates chunks from the original text based on defined parameters:

```python
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=100,
        )
```

The splitter would do the following:

```yml
Chunk 1: characters 0â€“1000
Chunk 2: characters 900â€“1900   (starts 100 chars earlier)
Chunk 3: characters 1800â€“2400
```

This visuals looks like:

```css
[-----1000----]
           [-----1000----]
                     [----end---]
```

NOTE: Overlap prevents meaning loss at chunk boundaries.

#### ğŸ’¡ **Example Chunk**:

```txt
[Chunk 1]
The hippocampus (pl.: hippocampi; via Latin from Greek á¼±Ï€Ï€ÏŒÎºÎ±Î¼Ï€Î¿Ï‚, 'seahorse'), also hippocampus proper, is a major component of the brain of humans and many other vertebrates. In the human brain the hippocampus, the dentate gyrus, and the subiculum are components of the hippocampal formation located in the limbic system. The hippocampus plays important roles in the consolidation of information from short-term memory to long-term memory, and in spatial memory that enables navigation. In humans and other primates the hippocampus is located in the archicortex, one of the three regions of allocortex, in each hemisphere with direct neural projections to, and reciprocal indirect projections from the neocortex. The hippocampus, as the medial pallium, is a structure found in all vertebrates.
```

#### ğŸ§  **Know-How**:

Chunk size does not affect the total text embedded, but it affects **retrieval quality** and **token costs** (smaller chunks = lower token input cost per retrieval).

**Why smaller chunks?** Better retrieval accuracy, less irrelevant text given to the model, lower token costs.  
**Typical Sizes**: Small (200â€“300 tokens), Medium (400â€“800 tokens), Large (1000+ tokens).

### C. EMBEDDING MODEL ğŸ”¬

#### ğŸ”§ **Example Tool**: `sentence-transformers/all-MiniLM-L6-v2`

- **Characteristics**: Outputs a **384-dimensional vector**, with values (floating-point numbers) typically **between -1 and 1**.  
  NOTE: Each embedding model has its own characteristics!

#### âš™ï¸ **How it Works**:

- **Chunks** are **TOKENIZED** (happens once per document ingestion).  
  **Why tokenize?** An embedding model's job is to turn text into a 'meaning vector.' To do that, it must first convert human-readable text into a numerical form the model understands.
- Those tokens are then **VECTORIZED**.  
  **Semantic Fingerprint**: Chunks that mean similar things end up with similar vectors and are stored in nearby positions in the vector space.

#### ğŸ’¡ **Example Vector**:

(Visual simulation of the first 30 values for Chunk 1)

```rust
Vector for Chunk 1 (384 dimensions):
[
0.12, -0.03, 0.44, -0.55, 0.09, 0.22, -0.41, 0.11,
0.88, -0.14, 0.02, 0.33, -0.32, 0.01, 0.47, -0.19,
0.50, 0.28, -0.22, 0.71, -0.07, -0.02, 0.31, -0.48,
0.16, 0.12, 0.09, -0.33, 0.44, 0.08,
... 354 more numbers ...
]
```

NOTE: Each chunk has 384 dimensions!

#### ğŸ§  **Know-How**:

**What are vectors?**  
 A vector is simply:

- a list of numbers
- representing the â€œmeaningâ€ of a piece of text
- living in a high-dimensional space - 384, 768, or 1536 dimensions

Small embeddings:

- Shorter vectors: 256, 384, 512 dimensions
- Faster to compute
- Cheaper
  Use in: classification, short text, small datasets

Large embeddings:

- Long vectors: 1024, 1536, 3072 dimensions
- More detailed representation of meaning
- Better semantic understanding
- More expensive
  Use in: large document sets, nuanced text, technical material

**What are embedding values? (The numbers between -1 and 1)**  
When we say an embedding is a vector like:  
[0.12, -0.55, 0.08, â€¦] (384 numbers)

Each number represents a coordinate in a huge mathematical space.  
Think of:  
A 2D point â†’ has 2 numbers: (x, y)  
A 3D point â†’ has 3 numbers: (x, y, z)  
A 384-dimensional point â†’ has 384 numbers

The model chooses 384 because this is the number of features it uses to represent meaning.

Why are the values between -1 and 1?  
This comes from neural network math:

- **tanh** activation â†’ output is always between -1 and 1
- Sometimes layer_norm â†’ also keeps values small  
  So the model forces all 384 values to fall into this range.

**What is a â€œsemantic fingerprintâ€?**  
Chunks that mean similar things end up in nearby positions.

Examples:  
Text A:  
The hippocampus is involved in memory consolidation.

Text B:  
Memory formation depends on the hippocampal region.

Their embeddings might look like:  
A = [0.10, -0.05, 0.48, ...]  
B = [0.11, -0.03, 0.47, ...]  
â†’these are almost identical â†’ meaning is similar.

But an unrelated text:  
The Eiffel Tower is located in Paris.

Might be:  
[ -0.88, 0.22, -0.05, ... ]  
â†’ completely different.

### D. VECTOR DB ğŸ—„ï¸

#### ğŸ”§ **Example Tool**: `Chroma`

#### âš™ï¸ **How it Works**:

- **Store**: Chroma stores the `id`, the `embedding` (the 384-dim vector), and `metadata` for each chunk.  
  Chroma stores embeddings in a columnar vector index on disk:

```python
collection/
â”œâ”€â”€ embeddings.bin (384 float32 values per row)
â”œâ”€â”€ ids.bin
â”œâ”€â”€ metadata.json
â””â”€â”€ documents.bin
```

- **Retrieval**: Chroma performs a **vector similarity** search (_cosine similarity_) between the query's embedding and every stored vector.

#### ğŸ’¡ **Example Document Structure (Chroma)**:

```json
{
"id": "chunk_1",
"embedding": [0.12, -0.03, 0.44, -0.55, ...],
"metadata": {
"source": "hippocampus_article",
"chunk": 1
},
"document": "The hippocampus (pl.: hippocampi; via Latin ...)"
}
```

<br></br>

## Architecture - Prototype vs Production ğŸ—ï¸

This section outlines the cost and complexity difference between the local study project and a scalable, cloud-hosted client solution.

| Feature âš™ï¸      | Prototype (Study Project)                               | ğŸš€ Production (Client Scenario)                                         |
| --------------- | ------------------------------------------------------- | ----------------------------------------------------------------------- |
| Data Source     | One-time downloaded `JSON` files (static snapshot).     | Dynamic fetch and update (`Cloud Scheduler` + Incremental Indexing).    |
| FastAPI Service | Stored and run locally.                                 | `Cloud Run` (Scalable, containerized, serverless hosting).              |
| Embedding Model | Local, free (`paraphrase-multilingual-MiniLM-L12-v2`).  | API-based (`text-embedding-3-small`) or self-hosted, dedicated service. |
| Vector DB       | Local, free (`Chroma`).                                 | Cloud-Hosted (`Qdrant`, `Pinecone`, or `hosted Chroma`).                |
| LLM Model       | OpenRouter free version (`google/gemma-3-27b-it:free`). | Commercial API (GPT-4o) or self-hosted, larger model.                   |

<br></br>

## ğŸ’° Operational Costs and Scalability Considerations

When scaling to production, four main cost categories appear. These costs behave differently: some scale with user traffic, others with data size.

### What Costs Exist in a System Like InsightHubAI or ContentNavigatorAI? ğŸ“

| Cost Category            | Behavior                         | Primary Drivers                                                                                          |
| ------------------------ | -------------------------------- | -------------------------------------------------------------------------------------------------------- |
| (A) Model (LLM) Costs    | Scales with User Traffic.        | _Input tokens_ (system prompt, user question, retrieved context) and _output tokens_ (generated answer). |
| (B) Embedding Costs      | Scales with Data Size.           | Total document token count; usually a cheap, one-time cost.                                              |
| (C) Vector DB Costs      | Scales with Data Size & Queries. | Storage size, retrieval operations (QPS), and hosting method.                                            |
| (D) Infrastructure Costs | Scales with System Runtime.      | Serverless hosting (e.g., Cloud Run), persistence (Cloud SQL), and automation (Scheduler/Tasks).         |

---

### How Do You Choose LLM, Embeddings, Vector DB? ğŸ“

#### Choose the LLM based on:

- language support (Hungarian â†’ important!)
- accuracy requirements
- price / token
- speed
- context window size

For Hungarian data, Gemma was a surprisingly good choice.

#### Choose Embeddings based on:

- language support (multilingual embeddings!)
- model dimension (256 vs. 384 vs. 768â€¦ affects cost + recall)
- compatibility with your vector store
- inference speed/price

Your switch from all-MiniLM â†’ multilingual-MiniLM was exactly the correct reasoning.

#### Choose Vector DB based on:

- dataset size
- retrieval latency needs
- budget
- need for cloud persistence
- features like metadata filtering

Chroma â†’ great for prototypes  
Pinecone â†’ good for scale  
Qdrant â†’ powerful + open-source  
Weaviate â†’ strong hybrid search

---

### LLM Query Flow and Token Cost (Visualized) ğŸ“

![How Tokens Flow Through a RAG System](./assets/ai_engineering_token-flow.drawio.png)

ğŸ–¼ï¸ open the image in a separate window:
[How Tokens Flow Through a RAG System](/assets/ai_engineering_token-flow.drawio.png)

ğŸªœ **Step A** â€” User Query

```txt
â€œWhere is the hippocampus located in humans?â€
```

1. Embedding Model

The query is embedded using the same model used for custom data preparation:

```ini
query_embedding = [0.05, -0.22, 0.91, ...] (384 dimensions)
```

2. LLM
   Tokenizes the user's question --> BILLABLE (user question)

ğŸªœ **Step B** â€” Vector similarity search

Chroma does:

```scss
cosine_similarity(query_embedding, chunk1_embedding)
```

Result might be:

```ini
similarity = 0.89 (high)
```

So Chroma returns the chunk itself:

```arduino
Retrieved text:
"In humans and other primates the hippocampus is located in the archicortex, ..."
```

ğŸªœ **Step C** â€” LLM receives the retrieved chunks (text) and system prompt:

Now, it has a whole picture:

```arduino
User question:
"Where is the hippocampus located in humans?"

Retrieved context:
"In humans and other primates the hippocampus is located in the archicortex..."

System prompt:
"Answer based only on the context."
```

ğŸªœ **Step D** â€” LLM generates the final answer

The LLM does not look at vectors.
It only uses the text retrieved using the vectors.

Example answer:

```txt
The hippocampus is located in the archicortex in each hemisphere of the human brain.
```

---

### Estimating Costs (Theoretical) ğŸ“

#### ğŸ¤” MLL

ğŸªœ **Step 1**â€Š-â€ŠEstimate retrieved context size  
Suppose the retriever returns 3 chunks, each roughly 120 words.  
That corresponds to approximately 180 tokens per chunk.

- Retrieved context: 3 Ã— 180 â‰ˆ 540 tokens

ğŸªœ **Step 2**â€Š-â€ŠMeasure system prompt size  
The system prompt defined in the code can be measured using an online token calculator.

- System prompt: â‰ˆ 200 tokens

ğŸªœ **Step 3**â€Š-â€ŠEstimate user input  
Try 3â€“5 hypothetical user questions (short, medium, conversational).

- User prompt: â‰ˆ 15â€“40 tokens

ğŸªœ **Step 4**â€Š-â€ŠEstimate output length

- Model answer: â‰ˆ 250 tokens

Theoretical token cost per request:

```rust
Total input tokens â‰ˆ system (200) + user (15) + retrieved docs (540)
â‰ˆ 755 tokens
Output tokens â‰ˆ 250 tokens
---
â‰ˆ 1005 tokens per request
```

If the LLM costs $0.50 per million input tokens and $1.50 per million output tokens:

```rust
Input cost: 755 / 1,000,000 _ $0.50 â‰ˆ $0.00038
Output cost: 250 / 1,000,000 _ $1.50 â‰ˆ $0.00038
Total per query â‰ˆ $0.00076
```

**_Total per query â‰ˆ $0.00076_**  
So one query costs **less than a tenth of a cent**.

With 50 requests per day:

- Daily cost â‰ˆ $0.038
- Monthly cost â‰ˆ $1.10

This estimation is intentionally approximate. Real systems vary depending on retriever behavior, prompt evolution, and user interaction patterns.

What this estimation does not capture (by design):

- retrievers returning more chunks
- long conversational history
- tool or function calls
- retries and error handling

These belong to production monitoring and post-launch optimization, not early-stage architectural reasoning.

#### ğŸ¤” EMBEDDING MODEL

Even though embeddings are "one-time," it is still useful to estimate their impact.

ğŸªœ **Step 1**â€Š-â€ŠEstimate total text size  
Example:
500 blog posts
Average length â‰ˆ 1,200 tokens

```ini
Total â‰ˆ 600,000 tokens
```

Exact tokenization is not required. Rough heuristics are sufficient:

- English â‰ˆ 4 characters per token
- Hungarian â‰ˆ 3â€“4 characters per token

These are practical approximations, not strict rules.

ğŸªœ **Step 2**â€Š-â€ŠEstimate embedding throughput  
For a typical MiniLM model:

- CPU: ~500â€“2,000 tokens/sec
- GPU: ~5,000â€“20,000 tokens/sec

```ini
600,000 tokens Ã· 2,000 tokens/sec â‰ˆ 5 minutes
```

ğŸªœ **Step 3**â€Š-â€ŠEstimate storage cost  
See the following paragraph.

#### ğŸ¤” VECTOR DB

The larger the dimension of the embedding (e.g., sentence-transformers/all-MiniLM-L6-v2 uses a 384-dimensional vector, other embedding models, e.g., 768), the larger the vector, the more space is taken in the DB.

Example:

- 100,000 chunks
- embedding dimension 384
- float32 numbers â†’ 4 bytes per number

Storage â‰ˆ

```ini
100,000 _ 384 _ 4 bytes = 153,600,000 bytes
â‰ˆ 153 MB
```

If you use 1536 dimensions (4Ã— bigger):

```ini
â‰ˆ 600 MB
```

If your DB is huge (millions of chunks), dimension matters a LOT for cost + performance.

#### ğŸ¤” CLOUD INFRASTRUCTURE

In a cloud setup, infrastructure costs may include:

- Cloud Run (serverless hosting)
- Cloud Storage
- Cloud Scheduler / Cloud Tasks
- networking and API calls

Serverless platforms significantly reduce costs by scaling to zero when idle.

---

### Putting Numbers on it: ContentNavigatorAI ğŸ’°

âš™ï¸ PROTOTYPE

- LLM: free-tier model
- Embedding model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
- Vector DB: local Chroma
- Hosting: local machine

Operational cost: effectively zero; only the developers' efforts would incur costs.

ğŸš€ PRODUCTION  
Architecture assumptions:

- LLM: GPT-4o
- Embedding model: text-embedding-3-small (API) or self-hosted MiniLM
- Vector DB: Qdrant (self-hosted on GCP) or Pinecone (managed)
- Hosting: Cloud Run (GCP)
- Traffic: ~50 queries/day

Estimated monthly costs (low traffic):

- LLM usage: ~$1â€“2
- Vector DB + storage: ~$5â€“10
- Infrastructure: a few dollars

Total: comfortably under $15/month + the developer's costs.
At higher traffic, LLM usage becomes the dominant costâ€Š-â€Šbut it scales linearly and predictably.

Overview:

| Scenario      | Operational Cost                       | Developer Cost                                                                          |
| ------------- | -------------------------------------- | --------------------------------------------------------------------------------------- |
| âš™ï¸ Prototype  | Effectively zero (local, free models). | 100% of the project's expense, covering design, engineering, and data preparation time. |
| ğŸš€ Production | Under $15/month (low traffic).         | Cost of building a robust, scalable, and optimized architecture.                        |

<br></br>
